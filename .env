# OPTIMIZED .env - Configuration for Ollama with GGUF Model
# =============================================================================
# Domain Configuration
ALLOWED_DOMAINS=localhost,127.0.0.1
# =============================================================================
# ü§ñ MODEL CONFIGURATION - OLLAMA OPTIMIZED
# =============================================================================
MODEL_URL=http://ollama:11434
MODEL_NAME=devstral
MODEL_GGUF_PATH=/root/.ollama/models/Devstral-Small-2507-Q4_K_M.gguf
# =============================================================================
# üéõÔ∏è MODEL PARAMETERS - OPTIMIZED FOR DEVSTRAL SMALL
# =============================================================================
# Core sampling parameters
MODEL_TEMPERATURE=0.7          # Good balance for code generation
MODEL_TOP_P=0.9               # Standard value, works well
MODEL_TOP_K=40                # Standard value, good diversity
MODEL_MIN_P=0.05              # Better than 0.0, helps with quality
MODEL_REPEAT_PENALTY=1.1      # Good default
MODEL_REPEAT_LAST_N=64        # Reasonable lookback
# Context and prediction settings - OPTIMIZED
MODEL_NUM_CTX=4096            # Increased from 512 - you have 22GB RAM!
MODEL_NUM_PREDICT=512         # Increased from 256 - allow longer responses
MODEL_SEED=0                  # 0 = random seed each time
# Hardware optimization - CORRECTED
OLLAMA_NUM_THREAD=3           # Matches your CPU allocation
OLLAMA_MMAP=false             # Server-level mmap setting - DISABLE FOR BETTER PERFORMANCE
OLLAMA_NUM_PARALLEL=1         # 1 chat = more stable with limited context
OLLAMA_GPU_LAYERS=41          # Conservative for 8GB VRAM - test and increase if stable (MAX 41 layers for this model)
# Ollama server configuration - OPTIMIZED
OLLAMA_HOST=0.0.0.0:11434
OLLAMA_ORIGINS=*
OLLAMA_MODELS=/root/.ollama/models
OLLAMA_MAX_LOADED_MODELS=1    # Keep at 1 for resource efficiency
OLLAMA_NOPRUNE=false
OLLAMA_FLASH_ATTENTION=true   # Changed to true - boolean, not 1
# Ollama initialization script settings
OLLAMA_MAX_RETRIES=30         # Reduced from 45 - faster startup
OLLAMA_RETRY_INTERVAL=15      # Reduced from 20 - faster startup
MODELFILE_PATH=/root/.ollama/scripts/Modelfile
# Ollama runtime settings
OLLAMA_KEEP_ALIVE=999h        # Keep model in memory for 999 hours
OLLAMA_USE_MMAP=false         # Disable memory mapping for better performance
# =============================================================================
# üê≥ DOCKER RESOURCES - OPTIMIZED FOR CPU+GPU HYBRID
# =============================================================================
# Hardware configuration
OLLAMA_CPU=3.0
OLLAMA_MEMORY=16G
REDIS_MEMORY=1G
REDIS_CPU=0.5
NGINX_MEMORY=1G
NGINX_CPU=0.5
# =============================================================================
# üì¶ REDIS & NETWORK
# =============================================================================
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_URL=redis://redis:6379/0
# =============================================================================
# üìù LOGGING
# =============================================================================
LOG_LEVEL=INFO
LOG_FORMAT=json